{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKFp778fHVAC4D4ouPvdw8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicben2/hgraph2graph/blob/main/Partb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0B3xfHARohk"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/wengong-jin/hgraph2graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd hgraph2graph"
      ],
      "metadata": {
        "id": "q5tpW3pWRyH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit networkx tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import random\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors, Draw, DataStructs\n",
        "from rdkit import DataStructs\n",
        "from hgraph import HierVAE, MolGraph, common_atom_vocab, Vocab, PairVocab\n",
        "from hgraph import MoleculeDataset\n",
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "id": "0LKK5liDR0lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "    self.vocab = None\n",
        "    self.atom_vocab = common_atom_vocab\n",
        "    self.rnn_type = 'LSTM'\n",
        "    self.hidden_size = 250\n",
        "    self.embed_size = 250\n",
        "    self.batch_size = 20\n",
        "    self.latent_size = 32\n",
        "    self.depthT = 15\n",
        "    self.depthG = 15\n",
        "    self.diterT = 1\n",
        "    self.diterG = 3\n",
        "    self.dropout = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "seed = 7\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#vocab\n",
        "vocab_path = 'data/chembl/vocab.txt'\n",
        "vocab = [x.strip(\"\\r\\n \").split() for x in open(vocab_path)]\n",
        "args.vocab = PairVocab(vocab, cuda=(device.type == 'cuda')) # FIX: Explicitly pass cuda argument\n",
        "\n",
        "def filter_for_vocab(smiles_list, vocab, max_atoms=50):\n",
        "    valid_smiles = []\n",
        "\n",
        "    for smi in tqdm(smiles_list, desc=\"Filtering for vocabulary\"):\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smi)\n",
        "            if mol is None:\n",
        "                continue\n",
        "            if mol.GetNumAtoms() > max_atoms:\n",
        "                continue\n",
        "\n",
        "            Chem.Kekulize(mol, clearAromaticFlags=False)\n",
        "\n",
        "            hmol = MolGraph(smi)\n",
        "            ok = True\n",
        "            for node, attr in hmol.mol_tree.nodes(data=True):\n",
        "                smiles_node = attr['smiles']\n",
        "                ok &= attr['label'] in vocab.vmap\n",
        "                for i, s in attr['inter_label']:\n",
        "                    ok &= (smiles_node, s) in vocab.vmap\n",
        "                if not ok:\n",
        "                    break\n",
        "\n",
        "            if ok:\n",
        "                valid_smiles.append(smi)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return valid_smiles\n",
        "\n",
        "with open('data/chembl/all.txt', 'r') as f:\n",
        "    all_smiles = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "valid_smiles = filter_for_vocab(candidate_smiles, vocab)\n",
        "\n",
        "#splitting\n",
        "TRAIN_SIZE = min(2000, len(valid_smiles) - 200)\n",
        "TEST_SIZE = 200\n",
        "train_smiles = valid_smiles[:TRAIN_SIZE]\n",
        "test_smiles = valid_smiles[TRAIN_SIZE:TRAIN_SIZE + TEST_SIZE]\n",
        "\n",
        "with open('train_final.txt', 'w') as f:\n",
        "    f.write('\\n'.join(train_smiles))\n",
        "with open('test_final.txt', 'w') as f:\n",
        "    f.write('\\n'.join(test_smiles))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "RQXrcJizR-1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hgraph.hgnn as hgnn_module\n",
        "\n",
        "def make_cuda_fixed(tensors):\n",
        "    tree_tensors, graph_tensors = tensors\n",
        "\n",
        "    def make_tensor(x):\n",
        "        if isinstance(x, torch. Tensor):\n",
        "            return x\n",
        "        elif isinstance(x, np.ndarray):\n",
        "            return torch.from_numpy(x)\n",
        "        else:\n",
        "            return torch.tensor(x)\n",
        "\n",
        "    tree_tensors = [make_tensor(x).to(device).long() for x in tree_tensors[:-1]] + [tree_tensors[-1]]\n",
        "    graph_tensors = [make_tensor(x).to(device).long() for x in graph_tensors[:-1]] + [graph_tensors[-1]]\n",
        "    return tree_tensors, graph_tensors\n",
        "\n",
        "hgnn_module.make_cuda = make_cuda_fixed\n",
        "import hgraph.nnutils as nnutils_module\n",
        "original_index_select_ND = nnutils_module.index_select_ND\n",
        "\n",
        "def index_select_ND_fixed(source, dim, index):\n",
        "    if not isinstance(index, torch.Tensor):\n",
        "        index = torch.tensor(index, device=source.device, dtype=torch.long)\n",
        "    return original_index_select_ND(source, dim, index)\n",
        "\n",
        "nnutils_module.index_select_ND = index_select_ND_fixed\n",
        "\n",
        "class TrainingArgs:\n",
        "    def __init__(self):\n",
        "        self.vocab = vocab\n",
        "        self.atom_vocab = common_atom_vocab\n",
        "        self.save_dir = 'checkpoints'\n",
        "\n",
        "        self.rnn_type = 'LSTM'\n",
        "        self.hidden_size = 250\n",
        "        self.embed_size = 250\n",
        "        self.batch_size = 20\n",
        "        self.latent_size = 32\n",
        "        self.depthT = 15\n",
        "        self.depthG = 15\n",
        "        self.diterT = 1\n",
        "        self.diterG = 3\n",
        "        self.dropout = 0.0\n",
        "\n",
        "        self.lr = 1e-3\n",
        "        self.clip_norm = 5.0\n",
        "        self.max_beta = 0.1\n",
        "\n",
        "        self.num_epochs = 6\n",
        "        self.print_iter = 20\n",
        "\n",
        "train_args = TrainingArgs()\n",
        "os.makedirs(train_args.save_dir, exist_ok=True)\n",
        "\n",
        "from hgraph import HierVAE\n",
        "\n",
        "model = HierVAE(train_args).to(device)\n",
        "print(\"Model #Params: %dK\" % (sum([x.nelement() for x in model.parameters()]) / 1000,))\n",
        "\n",
        "#weights\n",
        "for param in model.parameters():\n",
        "    if param.dim() == 1:\n",
        "        nn.init.constant_(param, 0)\n",
        "    else:\n",
        "        nn.init. xavier_normal_(param)\n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=train_args.lr)\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(train_args.save_dir, \"model.epoch_0\"))\n",
        "total_step = 0\n",
        "beta = 0.01\n",
        "\n",
        "for epoch in range(1, train_args. num_epochs + 1):\n",
        "    # Create dataset\n",
        "    dataset = MoleculeDataset(train_smiles, vocab, common_atom_vocab, train_args.batch_size)\n",
        "    loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=lambda x: x[0])\n",
        "\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_kl = []\n",
        "    epoch_wacc = []\n",
        "    epoch_tacc = []\n",
        "\n",
        "    successful_batches = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        total_step += 1\n",
        "\n",
        "        try:\n",
        "            model.zero_grad()\n",
        "            graphs, tensors, orders = batch\n",
        "            tree_tensors, graph_tensors = tensors\n",
        "\n",
        "            def to_tensor(x):\n",
        "                if isinstance(x, torch.Tensor):\n",
        "                    return x\n",
        "                elif isinstance(x, np.ndarray):\n",
        "                    return torch.from_numpy(x)\n",
        "                elif isinstance(x, list):\n",
        "                    return x\n",
        "                else:\n",
        "                    return torch.tensor(x)\n",
        "\n",
        "            tree_tensors = [to_tensor(x) for x in tree_tensors]\n",
        "            graph_tensors = [to_tensor(x) for x in graph_tensors]\n",
        "            tensors = (tree_tensors, graph_tensors)\n",
        "\n",
        "            #forward pass\n",
        "            loss, kl_div, wacc, iacc, tacc, sacc = model(graphs, tensors, orders, beta=beta)\n",
        "\n",
        "            #backward pass\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), train_args.clip_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            #record\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_kl.append(kl_div)\n",
        "            epoch_wacc.append(wacc * 100)\n",
        "            epoch_tacc.append(tacc * 100)\n",
        "\n",
        "            successful_batches += 1\n",
        "\n",
        "            if total_step % train_args.print_iter == 0 and len(epoch_losses) >= train_args.print_iter:\n",
        "                recent_loss = np.mean(epoch_losses[-train_args.print_iter:])\n",
        "                recent_kl = np.mean(epoch_kl[-train_args. print_iter:])\n",
        "                recent_wacc = np.mean(epoch_wacc[-train_args. print_iter:])\n",
        "                recent_tacc = np.mean(epoch_tacc[-train_args. print_iter:])\n",
        "                print(f\"\\n  [{total_step}] Loss: {recent_loss:.3f}, KL:  {recent_kl:.2f}, \"\n",
        "                      f\"Word Acc: {recent_wacc:.1f}%, Topo Acc: {recent_tacc:.1f}%\")\n",
        "\n",
        "    #beta\n",
        "    beta = min(train_args.max_beta, beta + 0.02)\n",
        "\n",
        "    #checkpoint\n",
        "    ckpt_path = os.path.join(train_args. save_dir, f\"model.epoch_{epoch}\")\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "\n",
        "    avg_loss = np.mean(epoch_losses) if epoch_losses else 0\n",
        "    print(f\"\\nEpoch {epoch}\")\n",
        "    print(f\"Avg Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "T43bmf2_SHE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tanimoto(smiles1, smiles2, radius=2, nBits=2048):\n",
        "    mol1 = Chem.MolFromSmiles(smiles1)\n",
        "    mol2 = Chem.MolFromSmiles(smiles2)\n",
        "\n",
        "    if mol1 is None or mol2 is None:\n",
        "        return None\n",
        "\n",
        "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, radius, nBits=nBits)\n",
        "    fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, radius, nBits=nBits)\n",
        "\n",
        "    return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
        "\n",
        "def is_exact_match(smiles1, smiles2):\n",
        "    mol1 = Chem.MolFromSmiles(smiles1)\n",
        "    mol2 = Chem.MolFromSmiles(smiles2)\n",
        "\n",
        "    if mol1 is None or mol2 is None:\n",
        "        return False\n",
        "\n",
        "    return Chem.MolToSmiles(mol1) == Chem.MolToSmiles(mol2)\n",
        "\n",
        "def is_valid_molecule(smiles):\n",
        "    if smiles is None or smiles == \"\":\n",
        "        return False\n",
        "    return Chem.MolFromSmiles(smiles) is not None\n",
        "\n",
        "def evaluate_model_on_test(model, test_smiles, vocab, atom_vocab, batch_size=20):\n",
        "    model.eval()\n",
        "\n",
        "    dataset = MoleculeDataset(test_smiles, vocab, atom_vocab, batch_size)\n",
        "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=lambda x: x[0])\n",
        "\n",
        "    exact_matches = 0\n",
        "    valid_outputs = 0\n",
        "    tanimoto_scores = []\n",
        "    n_evaluated = 0\n",
        "\n",
        "    actual_test = dataset.batches\n",
        "    flat_test = [smi for batch in actual_test for smi in batch]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_idx = 0\n",
        "        for batch in loader:\n",
        "            start_idx = batch_size * batch_idx\n",
        "            end_idx = min(batch_size * (batch_idx + 1), len(flat_test))\n",
        "            orig_batch = flat_test[start_idx:end_idx]\n",
        "\n",
        "            try:\n",
        "                dec_smiles = model.reconstruct(batch)\n",
        "\n",
        "                for orig, dec in zip(orig_batch, dec_smiles):\n",
        "                    n_evaluated += 1\n",
        "\n",
        "                    if is_valid_molecule(dec):\n",
        "                        valid_outputs += 1\n",
        "                        tanimoto = calculate_tanimoto(orig, dec)\n",
        "                        if tanimoto is not None:\n",
        "                            tanimoto_scores.append(tanimoto)\n",
        "\n",
        "                        if is_exact_match(orig, dec):\n",
        "                            exact_matches += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "    return {\n",
        "        'exact_match':  exact_matches / n_evaluated if n_evaluated > 0 else 0,\n",
        "        'mean_tanimoto':  np.mean(tanimoto_scores) if tanimoto_scores else 0,\n",
        "        'median_tanimoto':  np.median(tanimoto_scores) if tanimoto_scores else 0,\n",
        "        'validity':  valid_outputs / n_evaluated if n_evaluated > 0 else 0,\n",
        "        'n_evaluated': n_evaluated\n",
        "    }"
      ],
      "metadata": {
        "id": "7aneLO2cZIRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_epochs = []\n",
        "for f in os.listdir(train_args.save_dir):\n",
        "    if f.startswith('model.epoch_'):\n",
        "        epoch = int(f.split('_')[1])\n",
        "        checkpoint_epochs.append(epoch)\n",
        "\n",
        "checkpoint_epochs = sorted(checkpoint_epochs)\n",
        "print(f\"Found checkpoints: {checkpoint_epochs}\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for epoch in tqdm(checkpoint_epochs, desc=\"Evaluating checkpoints\"):\n",
        "    ckpt_path = os.path.join(train_args.save_dir, f\"model.epoch_{epoch}\")\n",
        "\n",
        "    eval_model = HierVAE(train_args).to(device)\n",
        "    eval_model.load_state_dict(torch.load(ckpt_path))\n",
        "    eval_model.eval()\n",
        "\n",
        "    metrics = evaluate_model_on_test(eval_model, test_smiles, vocab, common_atom_vocab)\n",
        "    metrics['epoch'] = epoch\n",
        "    results.append(metrics)\n",
        "\n",
        "    print(f\"Epoch {epoch}:  Exact={metrics['exact_match']:.3f}, \"\n",
        "          f\"Tanimoto={metrics['mean_tanimoto']:.3f}, Valid={metrics['validity']:.3f}\")\n",
        "\n",
        "    del eval_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "checkpoint_df = pd.DataFrame(results)\n",
        "checkpoint_df = checkpoint_df.sort_values('epoch').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "wrJTsBrRdUFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#graph\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "#exact match acc vs training step\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(checkpoint_df['epoch'], checkpoint_df['exact_match'] * 100,\n",
        "         'o-', color='steelblue', linewidth=2, markersize=10)\n",
        "ax1.set_xlabel('Training Epoch', fontsize=12)\n",
        "ax1.set_ylabel('Exact Match Accuracy (%)', fontsize=12)\n",
        "ax1.set_title('Exact Match Accuracy vs Training Progress', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xticks(checkpoint_df['epoch'])\n",
        "\n",
        "#tanimoto mean/median vs training step\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(checkpoint_df['epoch'], checkpoint_df['mean_tanimoto'],\n",
        "         's-', color='coral', linewidth=2, markersize=10, label='Mean')\n",
        "ax2.plot(checkpoint_df['epoch'], checkpoint_df['median_tanimoto'],\n",
        "         '^--', color='green', linewidth=2, markersize=10, label='Median')\n",
        "ax2.set_xlabel('Training Epoch', fontsize=12)\n",
        "ax2.set_ylabel('Tanimoto Similarity', fontsize=12)\n",
        "ax2.set_title('Tanimoto Similarity vs Training Progress', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xticks(checkpoint_df['epoch'])\n",
        "ax2.set_ylim(0, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('part_b_checkpoint_dynamics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A9-rHGn2doZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}