{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHcpYOB4SE6b04k69qzAwr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicben2/hgraph2graph/blob/main/Partc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOoQWPuFjUfW",
        "outputId": "2bd30158-f117-43e3-de26-897ea9d5094f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hgraph2graph'...\n",
            "remote: Enumerating objects: 364, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 364 (delta 113), reused 92 (delta 92), pack-reused 212 (from 1)\u001b[K\n",
            "Receiving objects: 100% (364/364), 153.12 MiB | 14.87 MiB/s, done.\n",
            "Resolving deltas: 100% (218/218), done.\n",
            "Updating files: 100% (89/89), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/wengong-jin/hgraph2graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd hgraph2graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwUSk5KrjepL",
        "outputId": "17e63314-5790-4161-aa31-00430cf97b15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hgraph2graph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit networkx tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors, Draw, DataStructs\n",
        "from rdkit import DataStructs\n",
        "from hgraph import HierVAE, MolGraph, common_atom_vocab, Vocab, PairVocab\n",
        "from hgraph import MoleculeDataset\n",
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FZFd3RdjgKK",
        "outputId": "564c528d-9e02-4d2a-b124-10d136bf29e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "    self.vocab = None\n",
        "    self.atom_vocab = common_atom_vocab\n",
        "    self.rnn_type = 'LSTM'\n",
        "    self.hidden_size = 250\n",
        "    self.embed_size = 250\n",
        "    self.batch_size = 20\n",
        "    self.latent_size = 32\n",
        "    self.depthT = 15\n",
        "    self.depthG = 15\n",
        "    self.diterT = 1\n",
        "    self.diterG = 3\n",
        "    self.dropout = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "seed = 7\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#vocab\n",
        "vocab_path = 'data/chembl/vocab.txt'\n",
        "vocab_list = [x.strip(\"\\r\\n \").split() for x in open(vocab_path)] # Renamed to vocab_list to avoid confusion\n",
        "args.vocab = PairVocab(vocab_list, cuda=(device.type == 'cuda')) # FIX: Explicitly pass cuda argument\n",
        "\n",
        "import hgraph.hgnn as hgnn\n",
        "\n",
        "def make_cuda_fixed(tensors):\n",
        "    tree_tensors, graph_tensors = tensors\n",
        "    def make_tensor(x):\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            return x\n",
        "        elif isinstance(x, np.ndarray):\n",
        "            return torch.from_numpy(x)\n",
        "        else:\n",
        "            return torch.tensor(x)\n",
        "    tree_tensors = [make_tensor(x).to(device).long() for x in tree_tensors[:-1]] + [tree_tensors[-1]]\n",
        "    graph_tensors = [make_tensor(x).to(device).long() for x in graph_tensors[:-1]] + [graph_tensors[-1]]\n",
        "    return tree_tensors, graph_tensors\n",
        "\n",
        "hgnn.make_cuda = make_cuda_fixed\n",
        "\n",
        "model = HierVAE(args).to(device)\n",
        "pretrained_path = 'ckpt/chembl-pretrained/model.ckpt'\n",
        "checkpoint = torch.load(pretrained_path, map_location=device)\n",
        "pretrained_state = checkpoint[0] if isinstance(checkpoint, tuple) else checkpoint\n",
        "model.load_state_dict(pretrained_state)\n",
        "model.eval()\n",
        "\n",
        "with open('data/chembl/all.txt', 'r') as f:\n",
        "    all_smiles = [line.strip() for line in f][: 2000]\n",
        "\n",
        "def filter_smiles(smiles_list, vocab, max_count=300):\n",
        "    valid = []\n",
        "    for smi in tqdm(smiles_list, desc=\"Begin filter\"):\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smi)\n",
        "            if mol is None or mol.GetNumAtoms() > 40 or mol.GetNumAtoms() < 5:\n",
        "                continue\n",
        "            Chem.Kekulize(mol, clearAromaticFlags=False)\n",
        "            hmol = MolGraph(smi)\n",
        "            ok = True\n",
        "            for node, attr in hmol.mol_tree.nodes(data=True):\n",
        "                smiles_node = attr['smiles']\n",
        "                ok &= attr['label'] in vocab.vmap\n",
        "                for i, s in attr['inter_label']:\n",
        "                    ok &= (smiles_node, s) in vocab.vmap\n",
        "                if not ok:\n",
        "                    break\n",
        "            if ok:\n",
        "                valid.append(smi)\n",
        "                if len(valid) >= max_count:\n",
        "                    break\n",
        "        except:\n",
        "            continue\n",
        "    return valid\n",
        "\n",
        "test_smiles = filter_smiles(all_smiles, vocab, max_count=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "vVjledvqjit0",
        "outputId": "6cce1e83-97ba-452f-c8dc-ec6e62554a3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1090635861.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mhgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cuda_fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHierVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mpretrained_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ckpt/chembl-pretrained/model.ckpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/hgraph2graph/hgraph/hgnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHierVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHierMPNEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matom_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepthT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepthG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHierMPNDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matom_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mditerT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mditerG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtie_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhmpn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/hgraph2graph/hgraph/encoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab, avocab, rnn_type, embed_size, hidden_size, depthT, depthG, dropout)\u001b[0m\n\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMolGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOND_LIST\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE_apos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mMolGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_POS\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m             )\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cuda_getDeviceCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             raise AssertionError(\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "from collections import defaultdict\n",
        "\n",
        "def categorize_progression_pattern(group_df):\n",
        "    \"\"\"\n",
        "    Categorize the decoding progression pattern for a molecule.\n",
        "\n",
        "    Returns:\n",
        "    - pattern: str indicating the progression type\n",
        "    - max_similarity: peak Tanimoto similarity achieved\n",
        "    - final_similarity: Tanimoto at the last step\n",
        "    - drop_magnitude: how much similarity dropped from peak (if applicable)\n",
        "    \"\"\"\n",
        "    if len(group_df) < 2:\n",
        "        return 'insufficient_data', None, None, None\n",
        "\n",
        "    tanimoto_values = group_df['tanimoto'].values\n",
        "    steps = group_df['step'].values\n",
        "\n",
        "    max_sim = tanimoto_values.max()\n",
        "    final_sim = tanimoto_values[-1]\n",
        "    max_idx = tanimoto_values.argmax()\n",
        "\n",
        "    # Calculate trend\n",
        "    correlation, _ = pearsonr(steps, tanimoto_values)\n",
        "\n",
        "    # Detect patterns\n",
        "    drop_magnitude = max_sim - final_sim\n",
        "\n",
        "    if correlation > 0.7:\n",
        "        pattern = 'monotonic_increase'  # Good progression\n",
        "    elif correlation < -0.3 and max_idx < len(tanimoto_values) - 2:\n",
        "        pattern = 'peak_then_drop'  # Failure mode\n",
        "    elif max_sim < 0.3:\n",
        "        pattern = 'never_increases'  # Severe failure\n",
        "    elif drop_magnitude > 0.2:\n",
        "        pattern = 'significant_drop'  # Moderate failure\n",
        "    elif abs(correlation) < 0.3:\n",
        "        pattern = 'erratic'  # Unstable progression\n",
        "    else:\n",
        "        pattern = 'gradual_increase'  # Normal progression\n",
        "\n",
        "    return pattern, max_sim, final_sim, drop_magnitude\n",
        "\n",
        "\n",
        "def analyze_failure_modes(decoding_df):\n",
        "    \"\"\"\n",
        "    Analyze and categorize failure modes in decoding progression.\n",
        "    \"\"\"\n",
        "    failure_analysis = []\n",
        "\n",
        "    for smi, group in decoding_df.groupby('original_smiles'):\n",
        "        group = group.sort_values('step')\n",
        "        pattern, max_sim, final_sim, drop_mag = categorize_progression_pattern(group)\n",
        "\n",
        "        failure_analysis.append({\n",
        "            'smiles': smi,\n",
        "            'pattern': pattern,\n",
        "            'max_similarity': max_sim,\n",
        "            'final_similarity': final_sim,\n",
        "            'drop_magnitude': drop_mag,\n",
        "            'num_steps': len(group),\n",
        "            'num_motifs': group['num_motifs'].iloc[0]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(failure_analysis)\n",
        "\n",
        "\n",
        "def plot_decoding_progression_analysis(decoding_df, failure_df, output_prefix='decoding_analysis'):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualization of decoding progression.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up the plotting style\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.rcParams['figure.figsize'] = (20, 12)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # 1. Overall progression: Tanimoto vs Step (averaged)\n",
        "    ax1 = fig.add_subplot(gs[0, :2])\n",
        "    step_stats = decoding_df.groupby('step').agg({\n",
        "        'tanimoto': ['mean', 'std', 'count']\n",
        "    }).reset_index()\n",
        "    step_stats.columns = ['step', 'mean_tanimoto', 'std_tanimoto', 'count']\n",
        "\n",
        "    ax1.plot(step_stats['step'], step_stats['mean_tanimoto'],\n",
        "             'o-', linewidth=2, markersize=8, label='Mean Tanimoto')\n",
        "    ax1.fill_between(step_stats['step'],\n",
        "                     step_stats['mean_tanimoto'] - step_stats['std_tanimoto'],\n",
        "                     step_stats['mean_tanimoto'] + step_stats['std_tanimoto'],\n",
        "                     alpha=0.3, label='±1 std')\n",
        "    ax1.set_xlabel('Decoding Step', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Tanimoto Similarity', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Average Decoding Progression Across All Molecules',\n",
        "                  fontsize=14, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Pattern distribution\n",
        "    ax2 = fig.add_subplot(gs[0, 2])\n",
        "    pattern_counts = failure_df['pattern'].value_counts()\n",
        "    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(pattern_counts)))\n",
        "    ax2.barh(range(len(pattern_counts)), pattern_counts.values, color=colors)\n",
        "    ax2.set_yticks(range(len(pattern_counts)))\n",
        "    ax2.set_yticklabels(pattern_counts.index, fontsize=10)\n",
        "    ax2.set_xlabel('Count', fontsize=11, fontweight='bold')\n",
        "    ax2.set_title('Progression Pattern Distribution', fontsize=12, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    # 3. Individual trajectories colored by pattern\n",
        "    ax3 = fig.add_subplot(gs[1, :])\n",
        "\n",
        "    pattern_colors = {\n",
        "        'monotonic_increase': 'green',\n",
        "        'gradual_increase': 'lightgreen',\n",
        "        'erratic': 'orange',\n",
        "        'significant_drop': 'red',\n",
        "        'peak_then_drop': 'darkred',\n",
        "        'never_increases': 'purple'\n",
        "    }\n",
        "\n",
        "    # Sample molecules from each pattern\n",
        "    for pattern in pattern_colors.keys():\n",
        "        pattern_molecules = failure_df[failure_df['pattern'] == pattern]['smiles'].values[:5]\n",
        "        for smi in pattern_molecules:\n",
        "            mol_data = decoding_df[decoding_df['original_smiles'] == smi].sort_values('step')\n",
        "            ax3.plot(mol_data['step'], mol_data['tanimoto'],\n",
        "                    alpha=0.6, linewidth=1.5, color=pattern_colors[pattern])\n",
        "\n",
        "    # Create legend\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [Line2D([0], [0], color=color, linewidth=2, label=pattern)\n",
        "                      for pattern, color in pattern_colors.items()]\n",
        "    ax3.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
        "    ax3.set_xlabel('Decoding Step', fontsize=12, fontweight='bold')\n",
        "    ax3.set_ylabel('Tanimoto Similarity', fontsize=12, fontweight='bold')\n",
        "    ax3.set_title('Individual Molecule Trajectories (5 per pattern)',\n",
        "                  fontsize=14, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Correlation: Fraction complete vs Tanimoto\n",
        "    ax4 = fig.add_subplot(gs[2, 0])\n",
        "    scatter_sample = decoding_df.sample(min(1000, len(decoding_df)))\n",
        "    ax4.hexbin(scatter_sample['fraction_complete'], scatter_sample['tanimoto'],\n",
        "              gridsize=30, cmap='YlOrRd', mincnt=1)\n",
        "    ax4.set_xlabel('Fraction Complete', fontsize=11, fontweight='bold')\n",
        "    ax4.set_ylabel('Tanimoto Similarity', fontsize=11, fontweight='bold')\n",
        "    ax4.set_title('Similarity vs. Completion', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # 5. Atom coverage vs Tanimoto\n",
        "    ax5 = fig.add_subplot(gs[2, 1])\n",
        "    ax5.hexbin(scatter_sample['atom_coverage'], scatter_sample['tanimoto'],\n",
        "              gridsize=30, cmap='YlGnBu', mincnt=1)\n",
        "    ax5.set_xlabel('Atom Coverage', fontsize=11, fontweight='bold')\n",
        "    ax5.set_ylabel('Tanimoto Similarity', fontsize=11, fontweight='bold')\n",
        "    ax5.set_title('Similarity vs. Atom Coverage', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # 6. Final similarity distribution by pattern\n",
        "    ax6 = fig.add_subplot(gs[2, 2])\n",
        "    failure_df_clean = failure_df[failure_df['final_similarity'].notna()]\n",
        "    patterns_to_plot = failure_df_clean['pattern'].value_counts().head(6).index\n",
        "    data_to_plot = [failure_df_clean[failure_df_clean['pattern'] == p]['final_similarity'].values\n",
        "                    for p in patterns_to_plot]\n",
        "\n",
        "    bp = ax6.boxplot(data_to_plot, labels=patterns_to_plot, patch_artist=True)\n",
        "    for patch, pattern in zip(bp['boxes'], patterns_to_plot):\n",
        "        patch.set_facecolor(pattern_colors.get(pattern, 'gray'))\n",
        "    ax6.set_xticklabels(patterns_to_plot, rotation=45, ha='right', fontsize=9)\n",
        "    ax6.set_ylabel('Final Tanimoto', fontsize=11, fontweight='bold')\n",
        "    ax6.set_title('Final Similarity by Pattern', fontsize=12, fontweight='bold')\n",
        "    ax6.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_prefix}_comprehensive.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Create a second figure for detailed failure mode examples\n",
        "    plot_failure_mode_examples(decoding_df, failure_df, output_prefix)\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_failure_mode_examples(decoding_df, failure_df, output_prefix='decoding_analysis'):\n",
        "    \"\"\"\n",
        "    Plot detailed examples of each failure mode.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    failure_patterns = ['peak_then_drop', 'never_increases', 'significant_drop',\n",
        "                       'erratic', 'monotonic_increase', 'gradual_increase']\n",
        "\n",
        "    for idx, pattern in enumerate(failure_patterns):\n",
        "        ax = axes[idx]\n",
        "        pattern_mols = failure_df[failure_df['pattern'] == pattern]['smiles'].values[:3]\n",
        "\n",
        "        for smi in pattern_mols:\n",
        "            mol_data = decoding_df[decoding_df['original_smiles'] == smi].sort_values('step')\n",
        "            ax.plot(mol_data['step'], mol_data['tanimoto'],\n",
        "                   'o-', linewidth=2, markersize=6, alpha=0.7, label=smi[:20]+'...')\n",
        "\n",
        "        ax.set_xlabel('Decoding Step', fontsize=11, fontweight='bold')\n",
        "        ax.set_ylabel('Tanimoto Similarity', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(f'Pattern: {pattern}', fontsize=12, fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend(fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_prefix}_failure_modes.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def generate_summary_statistics(decoding_df, failure_df):\n",
        "    \"\"\"\n",
        "    Generate comprehensive summary statistics.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"PARTIAL DECODING ANALYSIS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n1. OVERALL STATISTICS\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Total molecules analyzed: {decoding_df['original_smiles'].nunique()}\")\n",
        "    print(f\"Total decoding steps recorded: {len(decoding_df)}\")\n",
        "    print(f\"Average steps per molecule: {len(decoding_df) / decoding_df['original_smiles'].nunique():.2f}\")\n",
        "\n",
        "    print(\"\\n2. SIMILARITY METRICS\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Mean Tanimoto similarity: {decoding_df['tanimoto'].mean():.4f} ± {decoding_df['tanimoto'].std():.4f}\")\n",
        "    print(f\"Median Tanimoto similarity: {decoding_df['tanimoto'].median():.4f}\")\n",
        "    print(f\"Mean atom coverage: {decoding_df['atom_coverage'].mean():.4f} ± {decoding_df['atom_coverage'].std():.4f}\")\n",
        "\n",
        "    print(\"\\n3. PROGRESSION PATTERN DISTRIBUTION\")\n",
        "    print(\"-\" * 80)\n",
        "    pattern_dist = failure_df['pattern'].value_counts()\n",
        "    for pattern, count in pattern_dist.items():\n",
        "        pct = 100 * count / len(failure_df)\n",
        "        print(f\"{pattern:25s}: {count:4d} ({pct:5.2f}%)\")\n",
        "\n",
        "    print(\"\\n4. FAILURE MODE ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "    failure_patterns = ['peak_then_drop', 'never_increases', 'significant_drop', 'erratic']\n",
        "    total_failures = failure_df[failure_df['pattern'].isin(failure_patterns)]\n",
        "    print(f\"Total failure cases: {len(total_failures)} ({100*len(total_failures)/len(failure_df):.2f}%)\")\n",
        "\n",
        "    for pattern in failure_patterns:\n",
        "        pattern_data = failure_df[failure_df['pattern'] == pattern]\n",
        "        if len(pattern_data) > 0:\n",
        "            print(f\"\\n{pattern}:\")\n",
        "            print(f\"  Count: {len(pattern_data)}\")\n",
        "            print(f\"  Mean max similarity: {pattern_data['max_similarity'].mean():.4f}\")\n",
        "            print(f\"  Mean final similarity: {pattern_data['final_similarity'].mean():.4f}\")\n",
        "            if 'drop_magnitude' in pattern_data.columns:\n",
        "                print(f\"  Mean drop magnitude: {pattern_data['drop_magnitude'].mean():.4f}\")\n",
        "\n",
        "    print(\"\\n5. SUCCESS MODE ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "    success_patterns = ['monotonic_increase', 'gradual_increase']\n",
        "    for pattern in success_patterns:\n",
        "        pattern_data = failure_df[failure_df['pattern'] == pattern]\n",
        "        if len(pattern_data) > 0:\n",
        "            print(f\"\\n{pattern}:\")\n",
        "            print(f\"  Count: {len(pattern_data)}\")\n",
        "            print(f\"  Mean final similarity: {pattern_data['final_similarity'].mean():.4f}\")\n",
        "\n",
        "    print(\"\\n6. STEP-WISE PROGRESSION\")\n",
        "    print(\"-\" * 80)\n",
        "    step_progression = decoding_df.groupby('step')['tanimoto'].agg(['mean', 'std', 'count'])\n",
        "    print(step_progression.head(10))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    return {\n",
        "        'overall': {\n",
        "            'n_molecules': decoding_df['original_smiles'].nunique(),\n",
        "            'n_steps': len(decoding_df),\n",
        "            'mean_tanimoto': decoding_df['tanimoto'].mean(),\n",
        "            'std_tanimoto': decoding_df['tanimoto'].std()\n",
        "        },\n",
        "        'patterns': pattern_dist.to_dict(),\n",
        "        'failure_rate': len(total_failures) / len(failure_df)\n",
        "    }\n",
        "\n",
        "\n",
        "# Run the complete analysis\n",
        "print(\"Analyzing failure modes...\")\n",
        "failure_df = analyze_failure_modes(decoding_df)\n",
        "\n",
        "print(\"\\nGenerating visualizations...\")\n",
        "plot_decoding_progression_analysis(decoding_df, failure_df)\n",
        "\n",
        "print(\"\\nGenerating summary statistics...\")\n",
        "summary_stats = generate_summary_statistics(decoding_df, failure_df)"
      ],
      "metadata": {
        "id": "31hY6q4Dk1r3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}